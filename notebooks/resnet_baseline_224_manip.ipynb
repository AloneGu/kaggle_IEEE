{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Motorola-X': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'Samsung-Galaxy-S4': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'iPhone-6': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 'Samsung-Galaxy-Note3': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'Motorola-Nexus-6': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'LG-Nexus-5x': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 'Motorola-Droid-Maxx': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'HTC-1-M7': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'Sony-NEX-7': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'iPhone-4s': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import skimage.io\n",
    "import scipy.misc\n",
    "import imageio\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "list_classes = [\n",
    " 'Sony-NEX-7',\n",
    " 'Motorola-X',\n",
    " 'HTC-1-M7',\n",
    " 'Samsung-Galaxy-Note3',\n",
    " 'Motorola-Droid-Maxx',\n",
    " 'iPhone-4s',\n",
    " 'iPhone-6',\n",
    " 'LG-Nexus-5x',\n",
    " 'Samsung-Galaxy-S4',\n",
    " 'Motorola-Nexus-6']\n",
    "\n",
    "list_dict = {}\n",
    "for i in range(10):\n",
    "    key = list_classes[i]\n",
    "    v = [0,0,0,0,0,0,0,0,0,0]\n",
    "    v[i] = 1\n",
    "    list_dict[key] = v\n",
    "print(list_dict)\n",
    "\n",
    "train_dir = '../input/train'\n",
    "test_dir = '../input/test'\n",
    "test_files = sorted(glob.glob(test_dir+'/*'))\n",
    "train_files = sorted(glob.glob(train_dir+'/*/*'))\n",
    "train_data_cnt = len(train_files)\n",
    "BATCH_SIZE = 13\n",
    "CROP_LEN = 224\n",
    "\n",
    "def random_crop(im_array):\n",
    "    # crop\n",
    "    x_range = im_array.shape[0] - CROP_LEN\n",
    "    y_range = im_array.shape[1] - CROP_LEN\n",
    "    # print(x_range,y_range)\n",
    "    a = np.random.randint(x_range)\n",
    "    b = a + CROP_LEN\n",
    "    c = np.random.randint(y_range)\n",
    "    d = c + CROP_LEN\n",
    "    new_im_array = im_array[a:b,c:d,:]\n",
    "    return new_im_array\n",
    "\n",
    "def center_crop(im_array):\n",
    "    center_x = im_array.shape[0] // 2\n",
    "    center_y = im_array.shape[1] // 2\n",
    "    half_crop = CROP_LEN // 2\n",
    "    a = center_x - half_crop\n",
    "    b = a + CROP_LEN\n",
    "    c = center_y - half_crop\n",
    "    d = c + CROP_LEN\n",
    "    new_im_array = im_array[a:b,c:d,:]\n",
    "    return new_im_array\n",
    "    \n",
    "\n",
    "def random_manip(img,rnd):\n",
    "    if rnd == 0:\n",
    "        return img\n",
    "    \n",
    "    elif rnd == 1:\n",
    "        # gamma 0.8\n",
    "        new_img = skimage.exposure.adjust_gamma(img, gamma=0.8)\n",
    "        return new_img\n",
    "    \n",
    "    elif rnd == 2:\n",
    "        # gamma 1.2\n",
    "        new_img = skimage.exposure.adjust_gamma(img, gamma=1.2)\n",
    "        return new_img\n",
    "    \n",
    "    elif rnd == 3:\n",
    "        # jpeg 70\n",
    "        imageio.imwrite('/tmp/quality-70.jpg', img, quality=70)\n",
    "        try:\n",
    "            new_img = np.array(Image.open(('/tmp/quality-70.jpg')), dtype=\"uint8\")\n",
    "            os.remove('/tmp/quality-70.jpg')\n",
    "            return new_img\n",
    "        except:\n",
    "            return img\n",
    "    \n",
    "    elif rnd == 4:\n",
    "        # jpeg 90\n",
    "        imageio.imwrite('/tmp/quality-90.jpg', img, quality=90)\n",
    "        try:\n",
    "            new_img = np.array(Image.open(('/tmp/quality-90.jpg')), dtype=\"uint8\")\n",
    "            os.remove('/tmp/quality-90.jpg')\n",
    "            return new_img\n",
    "        except:\n",
    "            return img\n",
    "    \n",
    "    elif rnd == 5:\n",
    "        # 2x of original image\n",
    "        new_img = scipy.misc.imresize(img, 2.0, interp='bicubic')\n",
    "        return center_crop(new_img)\n",
    "    \n",
    "    elif rnd == 6:\n",
    "        new_img = scipy.misc.imresize(img, 1.5, interp='bicubic')\n",
    "        return center_crop(new_img)\n",
    "    \n",
    "    elif rnd == 7:\n",
    "        new_img = scipy.misc.imresize(img, 0.5, interp='bicubic')\n",
    "        return new_img\n",
    "    \n",
    "    elif rnd == 8:\n",
    "        new_img = scipy.misc.imresize(img, 0.8, interp='bicubic')\n",
    "        return new_img\n",
    "    \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_path, train_flag = True, fake_rnd = 0, rnd_range = 16):\n",
    "    # read img\n",
    "    im_array = np.array(Image.open((img_path)), dtype=\"uint8\")\n",
    "    \n",
    "    # train or valid\n",
    "    if train_flag is True:\n",
    "        # manip rnd\n",
    "        manip_rnd = np.random.randint(rnd_range) # only half manip, 1 to 8 use manip, if rnd_range = 64, 1/8 manip\n",
    "        #manip_rnd = 5\n",
    "        #print(manip_rnd,im_array.shape)\n",
    "        if manip_rnd < 7 or manip_rnd > 8 : \n",
    "            # no zoom in， 随机切出 224\n",
    "            im_array = random_crop(im_array)\n",
    "            # manip， 随机变化，如果放大则去中间部分\n",
    "            final_img = random_manip(im_array, manip_rnd)\n",
    "          \n",
    "        else:\n",
    "            # resize zoom out， 缩小\n",
    "            im_array = random_manip(im_array, manip_rnd)\n",
    "            # random crop on larger image， 随机切出 224\n",
    "            final_img = random_crop(im_array)\n",
    "        if final_img.shape[0]!=CROP_LEN or final_img.shape[1]!=CROP_LEN:\n",
    "            print('train',manip_rnd,final_img.shape,img_path,im_array.shape)\n",
    "    else:\n",
    "        # center crop for valid data\n",
    "        # manip rnd\n",
    "        manip_rnd = fake_rnd\n",
    "        if manip_rnd < 7 or manip_rnd > 8 :\n",
    "            # no zoom in， 取中间部分\n",
    "            im_array = center_crop(im_array)\n",
    "            # manip, 随机变化，如果放大则取中间部分\n",
    "            final_img = random_manip(im_array, manip_rnd)\n",
    "        else:\n",
    "            # resize zoom out, 先缩小图片\n",
    "            im_array = random_manip(im_array, manip_rnd)\n",
    "            # random crop on larger image, 取中间部分\n",
    "            final_img = center_crop(im_array)\n",
    "        if final_img.shape[0]!=CROP_LEN or final_img.shape[1]!=CROP_LEN:\n",
    "            print('valid',manip_rnd,final_img.shape,img_path,im_array.shape)\n",
    "    \n",
    "    final_img = final_img/127.5\n",
    "    final_img = final_img - 1.0\n",
    "    return final_img\n",
    "\n",
    "# 读取测试数据中间 224\n",
    "def get_test_img(img_path):\n",
    "    # read img\n",
    "    im_array = np.array(Image.open((img_path)), dtype=\"uint8\")\n",
    "    final_img = center_crop(im_array)\n",
    "    final_img = final_img/127.5\n",
    "    final_img = final_img - 1.0\n",
    "    return final_img\n",
    "\n",
    "def get_y(img_path):\n",
    "    n = img_path.split('/')[-2]\n",
    "    return list_dict[n]\n",
    "\n",
    "def data_gen(file_list, batch_size=BATCH_SIZE, train_flag = True, manip_rnd_range = 16):\n",
    "    curr_idx = 0\n",
    "    data_cnt = len(file_list)\n",
    "    fake_rnd = 0\n",
    "    while True:\n",
    "        if curr_idx + batch_size > data_cnt:\n",
    "            start_idx = data_cnt-batch_size\n",
    "            end_idx = data_cnt\n",
    "            curr_idx = 0\n",
    "            fake_rnd = 0 # reset fake_rnd, 13 不是 16 的整数倍, 从第二次 valid 开始变得 stable\n",
    "        else:\n",
    "            start_idx = curr_idx\n",
    "            end_idx = curr_idx + batch_size\n",
    "            curr_idx += batch_size\n",
    "        curr_fl = file_list[start_idx:end_idx]\n",
    "        if train_flag is True:\n",
    "            curr_x = [get_img(p,train_flag,fake_rnd=0,rnd_range=manip_rnd_range) for p in curr_fl]\n",
    "        else:\n",
    "            # make validation data stable\n",
    "            curr_x = []\n",
    "            for p in curr_fl:\n",
    "                tmp_img = get_img(p,train_flag,fake_rnd)\n",
    "                curr_x.append(tmp_img)\n",
    "                fake_rnd += 1\n",
    "                fake_rnd = fake_rnd % manip_rnd_range\n",
    "        curr_x = np.array(curr_x,dtype='float32')\n",
    "        curr_y = np.array([get_y(p) for p in curr_fl])\n",
    "        yield curr_x,curr_y\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train_files = shuffle(train_files,random_state=42)\n",
    "\n",
    "# 1/8 manip\n",
    "train_less_manip_gen = data_gen(train_files, BATCH_SIZE, True, 64)\n",
    "valid_less_manip_gen = data_gen(train_files, BATCH_SIZE, False, 64)\n",
    "# 1/2 manip\n",
    "train_gen = data_gen(train_files, BATCH_SIZE, True, 16)\n",
    "valid_gen = data_gen(train_files, BATCH_SIZE, False, 16)\n",
    "import math\n",
    "train_step = math.ceil(train_data_cnt/BATCH_SIZE)\n",
    "valid_step = train_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# def model\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best_resnet_manip.h5\n"
     ]
    }
   ],
   "source": [
    "model_p = 'best_resnet_manip.h5'\n",
    "\n",
    "if os.path.exists(model_p):\n",
    "    model = load_model(model_p)\n",
    "    K.set_value(model.optimizer.lr, 0.0001)\n",
    "    print('load',model_p)\n",
    "else:\n",
    "    from keras.applications.resnet50 import ResNet50\n",
    "    base_model = ResNet50(input_shape=(CROP_LEN,CROP_LEN,3),include_top=False,weights=None,pooling='max')\n",
    "    x = base_model.output\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    pred = Dense(10,activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input,outputs=pred)\n",
    "    print('get model')\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "212/212 [==============================] - 344s 2s/step - loss: 1.5185 - acc: 0.4771\n",
      "Epoch 2/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 1.5780 - acc: 0.4554\n",
      "Epoch 3/150\n",
      "212/212 [==============================] - 336s 2s/step - loss: 1.5250 - acc: 0.4663\n",
      "Epoch 4/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 1.5015 - acc: 0.4913\n",
      "Epoch 5/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 1.4715 - acc: 0.4895\n",
      "Epoch 6/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.4788 - acc: 0.4917\n",
      "Epoch 7/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.4279 - acc: 0.5000\n",
      "Epoch 8/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.4833 - acc: 0.4873\n",
      "Epoch 9/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 1.4284 - acc: 0.5127\n",
      "Epoch 10/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 1.4330 - acc: 0.5065\n",
      "Epoch 11/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.4020 - acc: 0.5076\n",
      "Epoch 12/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.3727 - acc: 0.5214\n",
      "Epoch 13/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 1.3553 - acc: 0.5493\n",
      "Epoch 14/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 1.3799 - acc: 0.5243\n",
      "Epoch 15/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 1.3600 - acc: 0.5305\n",
      "Epoch 16/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.3672 - acc: 0.5388\n",
      "Epoch 17/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 1.3388 - acc: 0.5381\n",
      "Epoch 18/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 1.3121 - acc: 0.5443\n",
      "Epoch 19/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 1.3175 - acc: 0.5417\n",
      "Epoch 20/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 1.3014 - acc: 0.5595\n",
      "Epoch 21/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 1.2801 - acc: 0.5613\n",
      "Epoch 22/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 1.3077 - acc: 0.5562\n",
      "Epoch 23/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 1.2569 - acc: 0.5675\n",
      "Epoch 24/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 1.2550 - acc: 0.5642\n",
      "Epoch 25/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 1.2398 - acc: 0.5700\n",
      "Epoch 26/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 1.2494 - acc: 0.5776\n",
      "Epoch 27/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 1.1828 - acc: 0.5987\n",
      "Epoch 28/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.2582 - acc: 0.5744\n",
      "Epoch 29/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 1.2017 - acc: 0.5933\n",
      "Epoch 30/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 1.1989 - acc: 0.5929\n",
      "Epoch 31/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 1.1800 - acc: 0.6016\n",
      "Epoch 32/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.1649 - acc: 0.6056\n",
      "Epoch 33/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 1.1815 - acc: 0.6005\n",
      "Epoch 34/150\n",
      "212/212 [==============================] - 322s 2s/step - loss: 1.1553 - acc: 0.6041\n",
      "Epoch 35/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 1.1835 - acc: 0.6030\n",
      "Epoch 36/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.1568 - acc: 0.6041\n",
      "Epoch 37/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 1.0963 - acc: 0.6197\n",
      "Epoch 38/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 1.1888 - acc: 0.5943\n",
      "Epoch 39/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 1.1146 - acc: 0.6187\n",
      "Epoch 40/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 1.1117 - acc: 0.6299\n",
      "Epoch 41/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1086 - acc: 0.6241\n",
      "Epoch 00041: reducing learning rate to 4.999999873689376e-05.\n",
      "212/212 [==============================] - 335s 2s/step - loss: 1.1091 - acc: 0.6237\n",
      "Epoch 42/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 1.0215 - acc: 0.6586\n",
      "Epoch 43/150\n",
      "212/212 [==============================] - 340s 2s/step - loss: 0.9680 - acc: 0.6742\n",
      "Epoch 44/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.9366 - acc: 0.6872\n",
      "Epoch 45/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.9793 - acc: 0.6673\n",
      "Epoch 46/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.9504 - acc: 0.6796\n",
      "Epoch 47/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.9122 - acc: 0.6952\n",
      "Epoch 48/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.9833 - acc: 0.6676\n",
      "Epoch 49/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.9393 - acc: 0.6756\n",
      "Epoch 50/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.9205 - acc: 0.6825\n",
      "Epoch 51/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9165 - acc: 0.7014\n",
      "Epoch 00051: reducing learning rate to 2.499999936844688e-05.\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.9173 - acc: 0.7010\n",
      "Epoch 52/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.8752 - acc: 0.7065\n",
      "Epoch 53/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.7983 - acc: 0.7395\n",
      "Epoch 54/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.7974 - acc: 0.7250\n",
      "Epoch 55/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.8684 - acc: 0.7148\n",
      "Epoch 56/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.8079 - acc: 0.7308\n",
      "Epoch 57/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.7773 - acc: 0.7402\n",
      "Epoch 58/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.7503 - acc: 0.7518\n",
      "Epoch 59/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.8145 - acc: 0.7329\n",
      "Epoch 60/150\n",
      "212/212 [==============================] - 324s 2s/step - loss: 0.7693 - acc: 0.7489\n",
      "Epoch 61/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 0.7738 - acc: 0.7467\n",
      "Epoch 62/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7898 - acc: 0.7313\n",
      "Epoch 00062: reducing learning rate to 1.249999968422344e-05.\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.7895 - acc: 0.7315\n",
      "Epoch 63/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.7412 - acc: 0.7573\n",
      "Epoch 64/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 0.7378 - acc: 0.7522\n",
      "Epoch 65/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6983 - acc: 0.7696\n",
      "Epoch 66/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.7201 - acc: 0.7587\n",
      "Epoch 67/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6943 - acc: 0.7678\n",
      "Epoch 68/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.7247 - acc: 0.7558\n",
      "Epoch 69/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.7291 - acc: 0.7602\n",
      "Epoch 70/150\n",
      "212/212 [==============================] - 336s 2s/step - loss: 0.7259 - acc: 0.7562\n",
      "Epoch 71/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7055 - acc: 0.7663\n",
      "Epoch 00071: reducing learning rate to 6.24999984211172e-06.\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.7046 - acc: 0.7667\n",
      "Epoch 72/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.7043 - acc: 0.7725\n",
      "Epoch 73/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.7016 - acc: 0.7696\n",
      "Epoch 74/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7000 - acc: 0.7718\n",
      "Epoch 00074: reducing learning rate to 3.12499992105586e-06.\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6998 - acc: 0.7714\n",
      "Epoch 75/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6619 - acc: 0.7779\n",
      "Epoch 76/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6664 - acc: 0.7812\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 328s 2s/step - loss: 0.6966 - acc: 0.7783\n",
      "Epoch 78/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6768 - acc: 0.7801\n",
      "Epoch 79/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6614 - acc: 0.7816\n",
      "Epoch 80/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6597 - acc: 0.7758\n",
      "Epoch 81/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6400 - acc: 0.7877\n",
      "Epoch 82/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6482 - acc: 0.7830\n",
      "Epoch 83/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6451 - acc: 0.7903\n",
      "Epoch 84/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6311 - acc: 0.7925\n",
      "Epoch 85/150\n",
      "212/212 [==============================] - 336s 2s/step - loss: 0.6712 - acc: 0.7783\n",
      "Epoch 86/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6856 - acc: 0.7808\n",
      "Epoch 87/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6650 - acc: 0.7772\n",
      "Epoch 88/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6667 - acc: 0.7773\n",
      "Epoch 00088: reducing learning rate to 1.56249996052793e-06.\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6691 - acc: 0.7772\n",
      "Epoch 89/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6421 - acc: 0.7899\n",
      "Epoch 90/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6301 - acc: 0.7903\n",
      "Epoch 91/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6489 - acc: 0.7830\n",
      "Epoch 92/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6512 - acc: 0.7881\n",
      "Epoch 93/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6184 - acc: 0.7943\n",
      "Epoch 94/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6784 - acc: 0.7754\n",
      "Epoch 95/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6368 - acc: 0.7925\n",
      "Epoch 96/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6107 - acc: 0.7943\n",
      "Epoch 97/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.6421 - acc: 0.7892\n",
      "Epoch 98/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6493 - acc: 0.7787\n",
      "Epoch 99/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6408 - acc: 0.7964\n",
      "Epoch 100/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6113 - acc: 0.8061\n",
      "Epoch 00100: reducing learning rate to 7.81249980263965e-07.\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6131 - acc: 0.8055\n",
      "Epoch 101/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6503 - acc: 0.7896\n",
      "Epoch 102/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6233 - acc: 0.8030\n",
      "Epoch 103/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6428 - acc: 0.7893\n",
      "Epoch 00103: reducing learning rate to 3.906249901319825e-07.\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6420 - acc: 0.7892\n",
      "Epoch 104/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6515 - acc: 0.7874\n",
      "Epoch 105/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6141 - acc: 0.7990\n",
      "Epoch 106/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6353 - acc: 0.7889\n",
      "Epoch 00106: reducing learning rate to 1.9531249506599124e-07.\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6358 - acc: 0.7888\n",
      "Epoch 107/150\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6333 - acc: 0.7943\n",
      "Epoch 108/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.6112 - acc: 0.8073\n",
      "Epoch 109/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6316 - acc: 0.7875\n",
      "Epoch 00109: reducing learning rate to 9.765624753299562e-08.\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.6322 - acc: 0.7870\n",
      "Epoch 110/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6503 - acc: 0.7837\n",
      "Epoch 111/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6345 - acc: 0.7866\n",
      "Epoch 112/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6802 - acc: 0.7674\n",
      "Epoch 00112: reducing learning rate to 4.882812376649781e-08.\n",
      "212/212 [==============================] - 333s 2s/step - loss: 0.6797 - acc: 0.7674\n",
      "Epoch 113/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6189 - acc: 0.7888\n",
      "Epoch 114/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6247 - acc: 0.7914\n",
      "Epoch 115/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6401 - acc: 0.7882\n",
      "Epoch 00115: reducing learning rate to 2.4414061883248905e-08.\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6401 - acc: 0.7881\n",
      "Epoch 116/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 0.6613 - acc: 0.7888\n",
      "Epoch 117/150\n",
      "212/212 [==============================] - 327s 2s/step - loss: 0.6443 - acc: 0.7787\n",
      "Epoch 118/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6425 - acc: 0.7860\n",
      "Epoch 00118: reducing learning rate to 1.2207030941624453e-08.\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6416 - acc: 0.7863\n",
      "Epoch 119/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6173 - acc: 0.7954\n",
      "Epoch 120/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6323 - acc: 0.7870\n",
      "Epoch 121/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6440 - acc: 0.7878\n",
      "Epoch 00121: reducing learning rate to 1e-08.\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6428 - acc: 0.7881\n",
      "Epoch 122/150\n",
      "212/212 [==============================] - 324s 2s/step - loss: 0.6415 - acc: 0.7808\n",
      "Epoch 123/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6396 - acc: 0.8030\n",
      "Epoch 124/150\n",
      "212/212 [==============================] - 330s 2s/step - loss: 0.6454 - acc: 0.7866\n",
      "Epoch 125/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.5947 - acc: 0.8041\n",
      "Epoch 126/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6338 - acc: 0.7939\n",
      "Epoch 127/150\n",
      "212/212 [==============================] - 324s 2s/step - loss: 0.6180 - acc: 0.7939\n",
      "Epoch 128/150\n",
      "212/212 [==============================] - 335s 2s/step - loss: 0.6440 - acc: 0.7939\n",
      "Epoch 129/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6244 - acc: 0.7975\n",
      "Epoch 130/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6336 - acc: 0.7921\n",
      "Epoch 131/150\n",
      "212/212 [==============================] - 326s 2s/step - loss: 0.6041 - acc: 0.8022\n",
      "Epoch 132/150\n",
      "212/212 [==============================] - 338s 2s/step - loss: 0.6446 - acc: 0.7866\n",
      "Epoch 133/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6320 - acc: 0.7859\n",
      "Epoch 134/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6513 - acc: 0.7877\n",
      "Epoch 135/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6158 - acc: 0.7954\n",
      "Epoch 136/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.5999 - acc: 0.8037\n",
      "Epoch 137/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6277 - acc: 0.7899\n",
      "Epoch 138/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6396 - acc: 0.7837\n",
      "Epoch 139/150\n",
      "212/212 [==============================] - 332s 2s/step - loss: 0.6144 - acc: 0.8070\n",
      "Epoch 140/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6382 - acc: 0.7874\n",
      "Epoch 141/150\n",
      "212/212 [==============================] - 336s 2s/step - loss: 0.6507 - acc: 0.7899\n",
      "Epoch 142/150\n",
      "212/212 [==============================] - 325s 2s/step - loss: 0.6767 - acc: 0.7714\n",
      "Epoch 143/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6345 - acc: 0.7888\n",
      "Epoch 144/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6361 - acc: 0.7917\n",
      "Epoch 145/150\n",
      "212/212 [==============================] - 334s 2s/step - loss: 0.6359 - acc: 0.8015\n",
      "Epoch 146/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6570 - acc: 0.7845\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 327s 2s/step - loss: 0.6576 - acc: 0.7859\n",
      "Epoch 148/150\n",
      "212/212 [==============================] - 328s 2s/step - loss: 0.6264 - acc: 0.7954\n",
      "Epoch 149/150\n",
      "212/212 [==============================] - 331s 2s/step - loss: 0.6124 - acc: 0.7935\n",
      "Epoch 150/150\n",
      "212/212 [==============================] - 329s 2s/step - loss: 0.6534 - acc: 0.7856\n",
      "Epoch 1/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1987 - acc: 0.5928Epoch 00001: val_acc improved from -inf to 0.51415, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 828s 4s/step - loss: 1.1969 - acc: 0.5929 - val_loss: 1.8037 - val_acc: 0.5142\n",
      "Epoch 2/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1811 - acc: 0.6081Epoch 00002: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 1.1810 - acc: 0.6081 - val_loss: 1.7485 - val_acc: 0.5025\n",
      "Epoch 3/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1869 - acc: 0.5950Epoch 00003: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 1.1883 - acc: 0.5947 - val_loss: 1.5871 - val_acc: 0.4964\n",
      "Epoch 4/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1433 - acc: 0.6187Epoch 00004: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 1.1415 - acc: 0.6197 - val_loss: 1.8598 - val_acc: 0.4597\n",
      "Epoch 5/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1884 - acc: 0.5921Epoch 00005: val_acc improved from 0.51415 to 0.53084, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 819s 4s/step - loss: 1.1902 - acc: 0.5914 - val_loss: 1.4591 - val_acc: 0.5308\n",
      "Epoch 6/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1728 - acc: 0.5957Epoch 00006: val_acc improved from 0.53084 to 0.53229, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 818s 4s/step - loss: 1.1732 - acc: 0.5951 - val_loss: 1.4713 - val_acc: 0.5323\n",
      "Epoch 7/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1575 - acc: 0.6099Epoch 00007: val_acc improved from 0.53229 to 0.59398, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 825s 4s/step - loss: 1.1574 - acc: 0.6099 - val_loss: 1.2493 - val_acc: 0.5940\n",
      "Epoch 8/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1327 - acc: 0.6128Epoch 00008: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 1.1310 - acc: 0.6132 - val_loss: 1.3376 - val_acc: 0.5602\n",
      "Epoch 9/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1303 - acc: 0.6201Epoch 00009: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 1.1291 - acc: 0.6205 - val_loss: 1.9206 - val_acc: 0.5080\n",
      "Epoch 10/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1486 - acc: 0.6219Epoch 00010: val_acc did not improve\n",
      "212/212 [==============================] - 815s 4s/step - loss: 1.1501 - acc: 0.6216 - val_loss: 1.4014 - val_acc: 0.5483\n",
      "Epoch 11/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1459 - acc: 0.6121Epoch 00011: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 1.1464 - acc: 0.6118 - val_loss: 1.3280 - val_acc: 0.5889\n",
      "Epoch 12/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1576 - acc: 0.6110Epoch 00012: val_acc did not improve\n",
      "212/212 [==============================] - 807s 4s/step - loss: 1.1580 - acc: 0.6107 - val_loss: 1.8360 - val_acc: 0.4488\n",
      "Epoch 13/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.1515 - acc: 0.6044Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 4.999999873689376e-05.\n",
      "212/212 [==============================] - 817s 4s/step - loss: 1.1510 - acc: 0.6049 - val_loss: 2.3547 - val_acc: 0.3835\n",
      "Epoch 14/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.0227 - acc: 0.6679Epoch 00014: val_acc did not improve\n",
      "212/212 [==============================] - 801s 4s/step - loss: 1.0221 - acc: 0.6680 - val_loss: 1.2575 - val_acc: 0.5903\n",
      "Epoch 15/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.0169 - acc: 0.6628Epoch 00015: val_acc improved from 0.59398 to 0.61829, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 816s 4s/step - loss: 1.0161 - acc: 0.6629 - val_loss: 1.2335 - val_acc: 0.6183\n",
      "Epoch 16/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9913 - acc: 0.6821Epoch 00016: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.9925 - acc: 0.6814 - val_loss: 1.2160 - val_acc: 0.6147\n",
      "Epoch 17/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.0134 - acc: 0.6533Epoch 00017: val_acc improved from 0.61829 to 0.68759, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 818s 4s/step - loss: 1.0130 - acc: 0.6531 - val_loss: 0.9785 - val_acc: 0.6876\n",
      "Epoch 18/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9864 - acc: 0.6682Epoch 00018: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.9863 - acc: 0.6680 - val_loss: 1.0014 - val_acc: 0.6680\n",
      "Epoch 19/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9859 - acc: 0.6631Epoch 00019: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.9848 - acc: 0.6636 - val_loss: 0.9776 - val_acc: 0.6800\n",
      "Epoch 20/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9684 - acc: 0.6865Epoch 00020: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.9685 - acc: 0.6858 - val_loss: 0.9509 - val_acc: 0.6851\n",
      "Epoch 21/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9651 - acc: 0.6697Epoch 00021: val_acc improved from 0.68759 to 0.71154, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.9649 - acc: 0.6698 - val_loss: 0.8800 - val_acc: 0.7115\n",
      "Epoch 22/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9509 - acc: 0.6752Epoch 00022: val_acc did not improve\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.9529 - acc: 0.6749 - val_loss: 1.2185 - val_acc: 0.6099\n",
      "Epoch 23/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9203 - acc: 0.6901Epoch 00023: val_acc did not improve\n",
      "212/212 [==============================] - 821s 4s/step - loss: 0.9193 - acc: 0.6905 - val_loss: 1.3821 - val_acc: 0.5573\n",
      "Epoch 24/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9870 - acc: 0.6635Epoch 00024: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.9885 - acc: 0.6629 - val_loss: 1.3376 - val_acc: 0.5762\n",
      "Epoch 25/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9400 - acc: 0.6806Epoch 00025: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.9400 - acc: 0.6803 - val_loss: 0.9003 - val_acc: 0.7079\n",
      "Epoch 26/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 1.0053 - acc: 0.6573Epoch 00026: val_acc did not improve\n",
      "212/212 [==============================] - 822s 4s/step - loss: 1.0045 - acc: 0.6575 - val_loss: 1.2686 - val_acc: 0.5991\n",
      "Epoch 27/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.9350 - acc: 0.6810Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 2.499999936844688e-05.\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.9378 - acc: 0.6800 - val_loss: 1.1078 - val_acc: 0.6684\n",
      "Epoch 28/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8899 - acc: 0.7040Epoch 00028: val_acc improved from 0.71154 to 0.73839, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.8886 - acc: 0.7039 - val_loss: 0.8454 - val_acc: 0.7384\n",
      "Epoch 29/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8971 - acc: 0.7018Epoch 00029: val_acc did not improve\n",
      "212/212 [==============================] - 807s 4s/step - loss: 0.8989 - acc: 0.7010 - val_loss: 0.9726 - val_acc: 0.6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8592 - acc: 0.7080Epoch 00030: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.8572 - acc: 0.7086 - val_loss: 1.1611 - val_acc: 0.6306\n",
      "Epoch 31/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8679 - acc: 0.7047Epoch 00031: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.8670 - acc: 0.7054 - val_loss: 0.8528 - val_acc: 0.7177\n",
      "Epoch 32/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8303 - acc: 0.7262Epoch 00032: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.8302 - acc: 0.7261 - val_loss: 0.9347 - val_acc: 0.7046\n",
      "Epoch 33/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8586 - acc: 0.7175Epoch 00033: val_acc improved from 0.73839 to 0.74274, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.8577 - acc: 0.7177 - val_loss: 0.7860 - val_acc: 0.7427\n",
      "Epoch 34/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8527 - acc: 0.7098Epoch 00034: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.8533 - acc: 0.7094 - val_loss: 0.8798 - val_acc: 0.7130\n",
      "Epoch 35/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8044 - acc: 0.7335Epoch 00035: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.8073 - acc: 0.7319 - val_loss: 0.9397 - val_acc: 0.6974\n",
      "Epoch 36/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8316 - acc: 0.7193Epoch 00036: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.8311 - acc: 0.7192 - val_loss: 0.9455 - val_acc: 0.6927\n",
      "Epoch 37/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8435 - acc: 0.7284Epoch 00037: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.8448 - acc: 0.7279 - val_loss: 0.9829 - val_acc: 0.6785\n",
      "Epoch 38/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8273 - acc: 0.7248Epoch 00038: val_acc did not improve\n",
      "212/212 [==============================] - 821s 4s/step - loss: 0.8284 - acc: 0.7242 - val_loss: 0.9458 - val_acc: 0.6934\n",
      "Epoch 39/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.8534 - acc: 0.7237Epoch 00039: val_acc did not improve\n",
      "\n",
      "Epoch 00039: reducing learning rate to 1.249999968422344e-05.\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.8528 - acc: 0.7235 - val_loss: 0.9518 - val_acc: 0.6890\n",
      "Epoch 40/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7680 - acc: 0.7393Epoch 00040: val_acc did not improve\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.7669 - acc: 0.7398 - val_loss: 0.8229 - val_acc: 0.7358\n",
      "Epoch 41/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7642 - acc: 0.7426Epoch 00041: val_acc improved from 0.74274 to 0.74637, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.7638 - acc: 0.7427 - val_loss: 0.7929 - val_acc: 0.7464\n",
      "Epoch 42/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7727 - acc: 0.7371Epoch 00042: val_acc improved from 0.74637 to 0.76887, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.7734 - acc: 0.7369 - val_loss: 0.7294 - val_acc: 0.7689\n",
      "Epoch 43/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7902 - acc: 0.7408Epoch 00043: val_acc improved from 0.76887 to 0.78229, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 820s 4s/step - loss: 0.7892 - acc: 0.7413 - val_loss: 0.7020 - val_acc: 0.7823\n",
      "Epoch 44/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7520 - acc: 0.7485Epoch 00044: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.7521 - acc: 0.7482 - val_loss: 0.7970 - val_acc: 0.7395\n",
      "Epoch 45/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7701 - acc: 0.7386Epoch 00045: val_acc did not improve\n",
      "212/212 [==============================] - 803s 4s/step - loss: 0.7694 - acc: 0.7388 - val_loss: 0.8228 - val_acc: 0.7297\n",
      "Epoch 46/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7643 - acc: 0.7521Epoch 00046: val_acc did not improve\n",
      "212/212 [==============================] - 812s 4s/step - loss: 0.7639 - acc: 0.7522 - val_loss: 0.6936 - val_acc: 0.7721\n",
      "Epoch 47/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7722 - acc: 0.7474Epoch 00047: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.7718 - acc: 0.7471 - val_loss: 0.8299 - val_acc: 0.7304\n",
      "Epoch 48/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7483 - acc: 0.7433Epoch 00048: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.7476 - acc: 0.7438 - val_loss: 1.0519 - val_acc: 0.6803\n",
      "Epoch 49/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7642 - acc: 0.7386Epoch 00049: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.7639 - acc: 0.7384 - val_loss: 0.7388 - val_acc: 0.7732\n",
      "Epoch 50/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7185 - acc: 0.7561Epoch 00050: val_acc did not improve\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.7200 - acc: 0.7554 - val_loss: 1.0058 - val_acc: 0.6865\n",
      "Epoch 51/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7338 - acc: 0.7477Epoch 00051: val_acc did not improve\n",
      "212/212 [==============================] - 821s 4s/step - loss: 0.7328 - acc: 0.7482 - val_loss: 0.7288 - val_acc: 0.7656\n",
      "Epoch 52/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7580 - acc: 0.7463Epoch 00052: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.7578 - acc: 0.7460 - val_loss: 0.7104 - val_acc: 0.7689\n",
      "Epoch 53/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7465 - acc: 0.7576Epoch 00053: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.7464 - acc: 0.7573 - val_loss: 0.6934 - val_acc: 0.7761\n",
      "Epoch 54/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7113 - acc: 0.7656Epoch 00054: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.7113 - acc: 0.7649 - val_loss: 0.8779 - val_acc: 0.7231\n",
      "Epoch 55/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7571 - acc: 0.7536Epoch 00055: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.7575 - acc: 0.7533 - val_loss: 0.7517 - val_acc: 0.7587\n",
      "Epoch 56/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7486 - acc: 0.7503Epoch 00056: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.7486 - acc: 0.7504 - val_loss: 0.7658 - val_acc: 0.7540\n",
      "Epoch 57/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7564 - acc: 0.7415Epoch 00057: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.7569 - acc: 0.7417 - val_loss: 0.7784 - val_acc: 0.7500\n",
      "Epoch 58/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7376 - acc: 0.7565Epoch 00058: val_acc did not improve\n",
      "\n",
      "Epoch 00058: reducing learning rate to 6.24999984211172e-06.\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.7367 - acc: 0.7565 - val_loss: 0.7624 - val_acc: 0.7547\n",
      "Epoch 59/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7446 - acc: 0.7536Epoch 00059: val_acc improved from 0.78229 to 0.79463, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 819s 4s/step - loss: 0.7455 - acc: 0.7536 - val_loss: 0.6343 - val_acc: 0.7946\n",
      "Epoch 60/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7210 - acc: 0.7612Epoch 00060: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.7202 - acc: 0.7616 - val_loss: 0.7544 - val_acc: 0.7620\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/212 [============================>.] - ETA: 1s - loss: 0.7046 - acc: 0.7627Epoch 00061: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.7041 - acc: 0.7627 - val_loss: 0.7158 - val_acc: 0.7779\n",
      "Epoch 62/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7157 - acc: 0.7641Epoch 00062: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.7161 - acc: 0.7642 - val_loss: 0.6763 - val_acc: 0.7830\n",
      "Epoch 63/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7205 - acc: 0.7517Epoch 00063: val_acc improved from 0.79463 to 0.79644, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 822s 4s/step - loss: 0.7191 - acc: 0.7522 - val_loss: 0.6409 - val_acc: 0.7964\n",
      "Epoch 64/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7155 - acc: 0.7557Epoch 00064: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.7164 - acc: 0.7558 - val_loss: 0.6514 - val_acc: 0.7910\n",
      "Epoch 65/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7073 - acc: 0.7656Epoch 00065: val_acc did not improve\n",
      "\n",
      "Epoch 00065: reducing learning rate to 3.12499992105586e-06.\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.7077 - acc: 0.7656 - val_loss: 0.6663 - val_acc: 0.7899\n",
      "Epoch 66/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6746 - acc: 0.7754Epoch 00066: val_acc improved from 0.79644 to 0.80443, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.6738 - acc: 0.7758 - val_loss: 0.6428 - val_acc: 0.8044\n",
      "Epoch 67/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7390 - acc: 0.7587Epoch 00067: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.7378 - acc: 0.7587 - val_loss: 0.6543 - val_acc: 0.7972\n",
      "Epoch 68/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6868 - acc: 0.7736Epoch 00068: val_acc improved from 0.80443 to 0.80552, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6875 - acc: 0.7736 - val_loss: 0.6344 - val_acc: 0.8055\n",
      "Epoch 69/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6900 - acc: 0.7678Epoch 00069: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6904 - acc: 0.7678 - val_loss: 0.6371 - val_acc: 0.7983\n",
      "Epoch 70/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7079 - acc: 0.7656Epoch 00070: val_acc did not improve\n",
      "\n",
      "Epoch 00070: reducing learning rate to 1.56249996052793e-06.\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.7088 - acc: 0.7656 - val_loss: 0.6358 - val_acc: 0.8041\n",
      "Epoch 71/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6754 - acc: 0.7783Epoch 00071: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.6753 - acc: 0.7783 - val_loss: 0.6305 - val_acc: 0.8048\n",
      "Epoch 72/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6795 - acc: 0.7758Epoch 00072: val_acc did not improve\n",
      "212/212 [==============================] - 799s 4s/step - loss: 0.6788 - acc: 0.7761 - val_loss: 0.6358 - val_acc: 0.7990\n",
      "Epoch 73/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6669 - acc: 0.7805Epoch 00073: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6659 - acc: 0.7808 - val_loss: 0.6259 - val_acc: 0.8019\n",
      "Epoch 74/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7156 - acc: 0.7659Epoch 00074: val_acc did not improve\n",
      "212/212 [==============================] - 823s 4s/step - loss: 0.7154 - acc: 0.7660 - val_loss: 0.6249 - val_acc: 0.8048\n",
      "Epoch 75/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7013 - acc: 0.7638Epoch 00075: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.7021 - acc: 0.7631 - val_loss: 0.6214 - val_acc: 0.8044\n",
      "Epoch 76/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6905 - acc: 0.7711Epoch 00076: val_acc did not improve\n",
      "212/212 [==============================] - 829s 4s/step - loss: 0.6893 - acc: 0.7718 - val_loss: 0.6247 - val_acc: 0.8044\n",
      "Epoch 77/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6940 - acc: 0.7685Epoch 00077: val_acc did not improve\n",
      "\n",
      "Epoch 00077: reducing learning rate to 7.81249980263965e-07.\n",
      "212/212 [==============================] - 800s 4s/step - loss: 0.6925 - acc: 0.7696 - val_loss: 0.6304 - val_acc: 0.8022\n",
      "Epoch 78/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6951 - acc: 0.7736Epoch 00078: val_acc improved from 0.80552 to 0.80552, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.6958 - acc: 0.7732 - val_loss: 0.6231 - val_acc: 0.8055\n",
      "Epoch 79/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6610 - acc: 0.7762Epoch 00079: val_acc improved from 0.80552 to 0.80914, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6595 - acc: 0.7769 - val_loss: 0.6172 - val_acc: 0.8091\n",
      "Epoch 80/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6848 - acc: 0.7670Epoch 00080: val_acc did not improve\n",
      "212/212 [==============================] - 807s 4s/step - loss: 0.6850 - acc: 0.7671 - val_loss: 0.6247 - val_acc: 0.8081\n",
      "Epoch 81/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7170 - acc: 0.7561Epoch 00081: val_acc did not improve\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.7174 - acc: 0.7562 - val_loss: 0.6205 - val_acc: 0.8062\n",
      "Epoch 82/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6668 - acc: 0.7856Epoch 00082: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.6655 - acc: 0.7863 - val_loss: 0.6249 - val_acc: 0.8022\n",
      "Epoch 83/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6901 - acc: 0.7703Epoch 00083: val_acc did not improve\n",
      "\n",
      "Epoch 00083: reducing learning rate to 3.906249901319825e-07.\n",
      "212/212 [==============================] - 806s 4s/step - loss: 0.6929 - acc: 0.7692 - val_loss: 0.6200 - val_acc: 0.8073\n",
      "Epoch 84/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7145 - acc: 0.7627Epoch 00084: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.7133 - acc: 0.7631 - val_loss: 0.6115 - val_acc: 0.8059\n",
      "Epoch 85/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6771 - acc: 0.7805Epoch 00085: val_acc improved from 0.80914 to 0.80914, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6751 - acc: 0.7812 - val_loss: 0.6173 - val_acc: 0.8091\n",
      "Epoch 86/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6677 - acc: 0.7802Epoch 00086: val_acc improved from 0.80914 to 0.81096, saving model to best_resnet_manip.h5\n",
      "\n",
      "Epoch 00086: reducing learning rate to 1.9531249506599124e-07.\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6680 - acc: 0.7801 - val_loss: 0.6169 - val_acc: 0.8110\n",
      "Epoch 87/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6826 - acc: 0.7696Epoch 00087: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6846 - acc: 0.7692 - val_loss: 0.6178 - val_acc: 0.8102\n",
      "Epoch 88/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6455 - acc: 0.7871Epoch 00088: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.6455 - acc: 0.7877 - val_loss: 0.6165 - val_acc: 0.8081\n",
      "Epoch 89/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6977 - acc: 0.7627Epoch 00089: val_acc did not improve\n",
      "212/212 [==============================] - 819s 4s/step - loss: 0.6968 - acc: 0.7631 - val_loss: 0.6188 - val_acc: 0.8088\n",
      "Epoch 90/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6812 - acc: 0.7725Epoch 00090: val_acc did not improve\n",
      "212/212 [==============================] - 822s 4s/step - loss: 0.6816 - acc: 0.7725 - val_loss: 0.6169 - val_acc: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6699 - acc: 0.7805Epoch 00091: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.6702 - acc: 0.7801 - val_loss: 0.6146 - val_acc: 0.8095\n",
      "Epoch 92/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7000 - acc: 0.7685Epoch 00092: val_acc did not improve\n",
      "\n",
      "Epoch 00092: reducing learning rate to 9.765624753299562e-08.\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6995 - acc: 0.7689 - val_loss: 0.6151 - val_acc: 0.8088\n",
      "Epoch 93/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6780 - acc: 0.7700Epoch 00093: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.6778 - acc: 0.7700 - val_loss: 0.6172 - val_acc: 0.8095\n",
      "Epoch 94/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6836 - acc: 0.7725Epoch 00094: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6836 - acc: 0.7725 - val_loss: 0.6162 - val_acc: 0.8088\n",
      "Epoch 95/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6780 - acc: 0.7794Epoch 00095: val_acc did not improve\n",
      "\n",
      "Epoch 00095: reducing learning rate to 4.882812376649781e-08.\n",
      "212/212 [==============================] - 821s 4s/step - loss: 0.6774 - acc: 0.7798 - val_loss: 0.6189 - val_acc: 0.8073\n",
      "Epoch 96/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6823 - acc: 0.7743Epoch 00096: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6814 - acc: 0.7747 - val_loss: 0.6177 - val_acc: 0.8077\n",
      "Epoch 97/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6857 - acc: 0.7700Epoch 00097: val_acc did not improve\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6855 - acc: 0.7703 - val_loss: 0.6173 - val_acc: 0.8084\n",
      "Epoch 98/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6877 - acc: 0.7736Epoch 00098: val_acc did not improve\n",
      "\n",
      "Epoch 00098: reducing learning rate to 2.4414061883248905e-08.\n",
      "212/212 [==============================] - 819s 4s/step - loss: 0.6875 - acc: 0.7736 - val_loss: 0.6177 - val_acc: 0.8073\n",
      "Epoch 99/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6808 - acc: 0.7791Epoch 00099: val_acc did not improve\n",
      "212/212 [==============================] - 810s 4s/step - loss: 0.6828 - acc: 0.7779 - val_loss: 0.6155 - val_acc: 0.8091\n",
      "Epoch 100/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6914 - acc: 0.7700Epoch 00100: val_acc did not improve\n",
      "212/212 [==============================] - 804s 4s/step - loss: 0.6913 - acc: 0.7700 - val_loss: 0.6178 - val_acc: 0.8066\n",
      "Epoch 101/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6858 - acc: 0.7743Epoch 00101: val_acc did not improve\n",
      "\n",
      "Epoch 00101: reducing learning rate to 1.2207030941624453e-08.\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6860 - acc: 0.7747 - val_loss: 0.6171 - val_acc: 0.8088\n",
      "Epoch 102/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7115 - acc: 0.7707Epoch 00102: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.7102 - acc: 0.7714 - val_loss: 0.6174 - val_acc: 0.8110\n",
      "Epoch 103/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6945 - acc: 0.7689Epoch 00103: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6930 - acc: 0.7692 - val_loss: 0.6173 - val_acc: 0.8088\n",
      "Epoch 104/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6965 - acc: 0.7590Epoch 00104: val_acc did not improve\n",
      "\n",
      "Epoch 00104: reducing learning rate to 1e-08.\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6979 - acc: 0.7591 - val_loss: 0.6166 - val_acc: 0.8088\n",
      "Epoch 105/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6673 - acc: 0.7711Epoch 00105: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6672 - acc: 0.7703 - val_loss: 0.6153 - val_acc: 0.8088\n",
      "Epoch 106/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6881 - acc: 0.7754Epoch 00106: val_acc did not improve\n",
      "212/212 [==============================] - 820s 4s/step - loss: 0.6880 - acc: 0.7750 - val_loss: 0.6196 - val_acc: 0.8099\n",
      "Epoch 107/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6689 - acc: 0.7849Epoch 00107: val_acc did not improve\n",
      "212/212 [==============================] - 822s 4s/step - loss: 0.6695 - acc: 0.7841 - val_loss: 0.6119 - val_acc: 0.8062\n",
      "Epoch 108/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6647 - acc: 0.7773Epoch 00108: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6637 - acc: 0.7776 - val_loss: 0.6144 - val_acc: 0.8070\n",
      "Epoch 109/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6875 - acc: 0.7754Epoch 00109: val_acc did not improve\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6876 - acc: 0.7754 - val_loss: 0.6141 - val_acc: 0.8081\n",
      "Epoch 110/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6777 - acc: 0.7718Epoch 00110: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6761 - acc: 0.7725 - val_loss: 0.6165 - val_acc: 0.8091\n",
      "Epoch 111/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7272 - acc: 0.7598Epoch 00111: val_acc did not improve\n",
      "212/212 [==============================] - 822s 4s/step - loss: 0.7284 - acc: 0.7594 - val_loss: 0.6123 - val_acc: 0.8077\n",
      "Epoch 112/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6539 - acc: 0.7845Epoch 00112: val_acc did not improve\n",
      "212/212 [==============================] - 808s 4s/step - loss: 0.6549 - acc: 0.7841 - val_loss: 0.6136 - val_acc: 0.8066\n",
      "Epoch 113/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6970 - acc: 0.7674Epoch 00113: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6975 - acc: 0.7671 - val_loss: 0.6168 - val_acc: 0.8070\n",
      "Epoch 114/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6773 - acc: 0.7707Epoch 00114: val_acc did not improve\n",
      "212/212 [==============================] - 825s 4s/step - loss: 0.6787 - acc: 0.7703 - val_loss: 0.6171 - val_acc: 0.8084\n",
      "Epoch 115/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6881 - acc: 0.7740Epoch 00115: val_acc did not improve\n",
      "212/212 [==============================] - 815s 4s/step - loss: 0.6894 - acc: 0.7739 - val_loss: 0.6144 - val_acc: 0.8077\n",
      "Epoch 116/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6668 - acc: 0.7681Epoch 00116: val_acc did not improve\n",
      "212/212 [==============================] - 812s 4s/step - loss: 0.6657 - acc: 0.7685 - val_loss: 0.6186 - val_acc: 0.8062\n",
      "Epoch 117/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6459 - acc: 0.7864Epoch 00117: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6460 - acc: 0.7863 - val_loss: 0.6186 - val_acc: 0.8073\n",
      "Epoch 118/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6668 - acc: 0.7732Epoch 00118: val_acc did not improve\n",
      "212/212 [==============================] - 825s 4s/step - loss: 0.6675 - acc: 0.7729 - val_loss: 0.6169 - val_acc: 0.8073\n",
      "Epoch 119/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6839 - acc: 0.7787Epoch 00119: val_acc did not improve\n",
      "212/212 [==============================] - 820s 4s/step - loss: 0.6845 - acc: 0.7787 - val_loss: 0.6159 - val_acc: 0.8073\n",
      "Epoch 120/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6717 - acc: 0.7743Epoch 00120: val_acc did not improve\n",
      "212/212 [==============================] - 820s 4s/step - loss: 0.6741 - acc: 0.7736 - val_loss: 0.6168 - val_acc: 0.8070\n",
      "Epoch 121/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6778 - acc: 0.7692Epoch 00121: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.6765 - acc: 0.7700 - val_loss: 0.6159 - val_acc: 0.8077\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/212 [============================>.] - ETA: 1s - loss: 0.7077 - acc: 0.7667Epoch 00122: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.7083 - acc: 0.7663 - val_loss: 0.6161 - val_acc: 0.8062\n",
      "Epoch 123/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6605 - acc: 0.7791Epoch 00123: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.6612 - acc: 0.7783 - val_loss: 0.6138 - val_acc: 0.8073\n",
      "Epoch 124/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6776 - acc: 0.7721Epoch 00124: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.6783 - acc: 0.7721 - val_loss: 0.6148 - val_acc: 0.8102\n",
      "Epoch 125/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6919 - acc: 0.7743Epoch 00125: val_acc did not improve\n",
      "212/212 [==============================] - 812s 4s/step - loss: 0.6921 - acc: 0.7743 - val_loss: 0.6126 - val_acc: 0.8088\n",
      "Epoch 126/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6788 - acc: 0.7667Epoch 00126: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.6774 - acc: 0.7674 - val_loss: 0.6152 - val_acc: 0.8066\n",
      "Epoch 127/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.7182 - acc: 0.7619Epoch 00127: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.7186 - acc: 0.7620 - val_loss: 0.6174 - val_acc: 0.8088\n",
      "Epoch 128/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6947 - acc: 0.7681Epoch 00128: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6929 - acc: 0.7689 - val_loss: 0.6165 - val_acc: 0.8055\n",
      "Epoch 129/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6509 - acc: 0.7849Epoch 00129: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6504 - acc: 0.7852 - val_loss: 0.6192 - val_acc: 0.8066\n",
      "Epoch 130/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6522 - acc: 0.7776Epoch 00130: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6524 - acc: 0.7772 - val_loss: 0.6190 - val_acc: 0.8077\n",
      "Epoch 131/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6809 - acc: 0.7747Epoch 00131: val_acc did not improve\n",
      "212/212 [==============================] - 816s 4s/step - loss: 0.6810 - acc: 0.7747 - val_loss: 0.6155 - val_acc: 0.8091\n",
      "Epoch 132/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6892 - acc: 0.7659Epoch 00132: val_acc did not improve\n",
      "212/212 [==============================] - 820s 4s/step - loss: 0.6895 - acc: 0.7652 - val_loss: 0.6161 - val_acc: 0.8106\n",
      "Epoch 133/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6904 - acc: 0.7696Epoch 00133: val_acc did not improve\n",
      "212/212 [==============================] - 823s 4s/step - loss: 0.6890 - acc: 0.7703 - val_loss: 0.6169 - val_acc: 0.8081\n",
      "Epoch 134/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6937 - acc: 0.7550Epoch 00134: val_acc did not improve\n",
      "212/212 [==============================] - 805s 4s/step - loss: 0.6958 - acc: 0.7544 - val_loss: 0.6147 - val_acc: 0.8066\n",
      "Epoch 135/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6636 - acc: 0.7787Epoch 00135: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6628 - acc: 0.7787 - val_loss: 0.6142 - val_acc: 0.8095\n",
      "Epoch 136/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6648 - acc: 0.7831Epoch 00136: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6638 - acc: 0.7834 - val_loss: 0.6173 - val_acc: 0.8041\n",
      "Epoch 137/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6833 - acc: 0.7641Epoch 00137: val_acc did not improve\n",
      "212/212 [==============================] - 811s 4s/step - loss: 0.6835 - acc: 0.7638 - val_loss: 0.6136 - val_acc: 0.8081\n",
      "Epoch 138/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6687 - acc: 0.7769Epoch 00138: val_acc did not improve\n",
      "212/212 [==============================] - 807s 4s/step - loss: 0.6694 - acc: 0.7772 - val_loss: 0.6140 - val_acc: 0.8088\n",
      "Epoch 139/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6845 - acc: 0.7791Epoch 00139: val_acc did not improve\n",
      "212/212 [==============================] - 812s 4s/step - loss: 0.6860 - acc: 0.7776 - val_loss: 0.6182 - val_acc: 0.8073\n",
      "Epoch 140/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6877 - acc: 0.7681Epoch 00140: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6874 - acc: 0.7681 - val_loss: 0.6176 - val_acc: 0.8062\n",
      "Epoch 141/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6756 - acc: 0.7765Epoch 00141: val_acc did not improve\n",
      "212/212 [==============================] - 807s 4s/step - loss: 0.6739 - acc: 0.7772 - val_loss: 0.6136 - val_acc: 0.8077\n",
      "Epoch 142/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6686 - acc: 0.7711Epoch 00142: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.6679 - acc: 0.7718 - val_loss: 0.6178 - val_acc: 0.8088\n",
      "Epoch 143/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6759 - acc: 0.7773Epoch 00143: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6759 - acc: 0.7776 - val_loss: 0.6167 - val_acc: 0.8081\n",
      "Epoch 144/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6718 - acc: 0.7725Epoch 00144: val_acc did not improve\n",
      "212/212 [==============================] - 818s 4s/step - loss: 0.6715 - acc: 0.7729 - val_loss: 0.6149 - val_acc: 0.8081\n",
      "Epoch 145/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6614 - acc: 0.7813Epoch 00145: val_acc did not improve\n",
      "212/212 [==============================] - 817s 4s/step - loss: 0.6615 - acc: 0.7808 - val_loss: 0.6211 - val_acc: 0.8088\n",
      "Epoch 146/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6820 - acc: 0.7816Epoch 00146: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6807 - acc: 0.7823 - val_loss: 0.6163 - val_acc: 0.8081\n",
      "Epoch 147/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6757 - acc: 0.7692Epoch 00147: val_acc did not improve\n",
      "212/212 [==============================] - 814s 4s/step - loss: 0.6754 - acc: 0.7692 - val_loss: 0.6174 - val_acc: 0.8081\n",
      "Epoch 148/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6969 - acc: 0.7685Epoch 00148: val_acc did not improve\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6978 - acc: 0.7685 - val_loss: 0.6175 - val_acc: 0.8073\n",
      "Epoch 149/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6978 - acc: 0.7758Epoch 00149: val_acc did not improve\n",
      "212/212 [==============================] - 809s 4s/step - loss: 0.6992 - acc: 0.7754 - val_loss: 0.6131 - val_acc: 0.8066\n",
      "Epoch 150/150\n",
      "211/212 [============================>.] - ETA: 1s - loss: 0.6796 - acc: 0.7783Epoch 00150: val_acc improved from 0.81096 to 0.81168, saving model to best_resnet_manip.h5\n",
      "212/212 [==============================] - 813s 4s/step - loss: 0.6790 - acc: 0.7779 - val_loss: 0.6134 - val_acc: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67b1aad978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "from keras import backend as K\n",
    "#K.set_value(model.optimizer.lr, 0.0005)   \n",
    "\n",
    "model_chk = ModelCheckpoint(filepath=model_p, monitor='val_acc', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, verbose=1,\n",
    "                              patience=3, min_lr=0.00000001)\n",
    "# less manip\n",
    "model.fit_generator(train_less_manip_gen,\n",
    "          steps_per_epoch = train_step,\n",
    "          epochs=150,\n",
    "          callbacks=[reduce_lr]\n",
    "         )\n",
    "model.save('less_manip_resnet.h5')\n",
    "# half manip\n",
    "K.set_value(model.optimizer.lr, 0.0001)\n",
    "model.fit_generator(train_gen,\n",
    "          steps_per_epoch = train_step,\n",
    "          epochs=150,\n",
    "          validation_data = valid_gen,\n",
    "          validation_steps = valid_step,\n",
    "          callbacks=[model_chk,reduce_lr]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.27566739e-03   6.45841509e-02   2.97193159e-03   1.05084278e-01\n",
      "    5.25144441e-03   9.82298225e-05   1.26907513e-01   1.79201141e-01\n",
      "    7.80865699e-02   4.35539067e-01]\n",
      " [  4.25943546e-02   1.17025222e-03   8.55111610e-03   1.15131226e-03\n",
      "    9.80214961e-03   7.80220687e-01   6.19331971e-02   5.46191819e-02\n",
      "    1.42819155e-03   3.85296680e-02]\n",
      " [  2.61258124e-03   5.43727147e-05   1.01568773e-02   7.53344139e-05\n",
      "    1.42839548e-04   3.19137424e-03   9.76629853e-01   5.54667879e-03\n",
      "    5.18945162e-04   1.07117300e-03]\n",
      " [  2.36630640e-05   9.30087059e-04   1.88503094e-04   1.22046387e-02\n",
      "    2.80866770e-05   2.56331696e-04   2.58070906e-03   4.07045186e-02\n",
      "    9.38252628e-01   4.83088940e-03]\n",
      " [  4.04075280e-08   9.92269397e-01   1.21473153e-04   2.83636176e-03\n",
      "    1.39051481e-04   3.74372961e-04   4.34334106e-05   1.24712766e-03\n",
      "    2.85401498e-03   1.14670707e-04]]\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(model_p)\n",
    "test_y = []\n",
    "for img_p in test_files:\n",
    "    tmp_x = get_test_img(img_p)\n",
    "    tmp_y = best_model.predict(np.array([tmp_x]))[0]\n",
    "    test_y.append(tmp_y)\n",
    "\n",
    "test_y = np.array(test_y)\n",
    "print(test_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   fname    camera\n",
      "0  img_0002a04_manip.tif  iPhone-6\n",
      "1  img_001e31c_unalt.tif  iPhone-6\n",
      "2  img_00275cf_manip.tif  iPhone-6\n",
      "3  img_0034113_unalt.tif  iPhone-6\n",
      "4  img_00344b7_unalt.tif  iPhone-6\n",
      "                   fname             camera\n",
      "0  img_0002a04_manip.tif   Motorola-Nexus-6\n",
      "1  img_001e31c_unalt.tif          iPhone-4s\n",
      "2  img_00275cf_manip.tif           iPhone-6\n",
      "3  img_0034113_unalt.tif  Samsung-Galaxy-S4\n",
      "4  img_00344b7_unalt.tif         Motorola-X\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_res = np.argmax(test_y,axis=1)\n",
    "y_res = [list_classes[i] for i in y_res]\n",
    "df = pd.read_csv('../input/sample_submission.csv')\n",
    "print(df.head())\n",
    "f_name = [p.split('/')[-1] for p in test_files]\n",
    "df['fname'] = f_name\n",
    "df['camera'] = y_res\n",
    "print(df.head())\n",
    "df.to_csv('../results/resnet_manip.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 224, 224, 3)\n",
      "(5, 10)\n",
      "[ 0.2845816   0.23909293  0.26623553  0.34260133  0.09864068  0.0268707\n",
      "  1.54612064  0.58612871  0.63078678  0.97894108]\n",
      "[[  2.84581602e-01   2.39092931e-01   2.66235530e-01   3.42601329e-01\n",
      "    9.86406803e-02   2.68707015e-02   1.54612064e+00   5.86128712e-01\n",
      "    6.30786777e-01   9.78941083e-01]\n",
      " [  2.06692398e-01   5.27044497e-02   1.13730408e-01   5.18424669e-03\n",
      "    1.58593237e-01   3.42289448e+00   2.87252784e-01   2.35980481e-01\n",
      "    1.02664940e-02   5.06700754e-01]\n",
      " [  4.61556017e-02   3.98935564e-03   1.44330502e-01   8.28277087e-04\n",
      "    4.16976353e-03   5.28063672e-03   4.75948572e+00   3.32672596e-02\n",
      "    1.05032523e-03   1.44221005e-03]\n",
      " [  4.15783259e-04   1.32368997e-01   5.43155475e-03   1.04007564e-01\n",
      "    2.21762853e-03   3.74690769e-03   3.33249494e-02   1.33390844e-01\n",
      "    4.48504543e+00   1.00050122e-01]\n",
      " [  5.99006162e-05   4.59383774e+00   4.23500687e-02   6.84137195e-02\n",
      "    2.92387325e-03   1.01464251e-02   1.36077078e-03   3.09350751e-02\n",
      "    2.43102655e-01   6.86986279e-03]]\n",
      "                   fname             camera\n",
      "0  img_0002a04_manip.tif           iPhone-6\n",
      "1  img_001e31c_unalt.tif          iPhone-4s\n",
      "2  img_00275cf_manip.tif           iPhone-6\n",
      "3  img_0034113_unalt.tif  Samsung-Galaxy-S4\n",
      "4  img_00344b7_unalt.tif         Motorola-X\n"
     ]
    }
   ],
   "source": [
    "# TTA\n",
    "def get_tta_img(img_path):\n",
    "    im_array = np.array(Image.open((img_path)), dtype=\"uint8\")\n",
    "    im_array = im_array/127.5\n",
    "    im_array = im_array - 1.0\n",
    "    img_list = []\n",
    "    img_list.append(center_crop(im_array))\n",
    "    img_list.append(im_array[0:CROP_LEN,0:CROP_LEN,:])\n",
    "    img_list.append(im_array[0:CROP_LEN,512-CROP_LEN:512,:])\n",
    "    img_list.append(im_array[512-CROP_LEN:512,0:CROP_LEN,:])\n",
    "    img_list.append(im_array[512-CROP_LEN:512,512-CROP_LEN:512,:])\n",
    "    return np.array(img_list)\n",
    "tta_img = get_tta_img(test_files[0])\n",
    "print(tta_img.shape)\n",
    "tta_res = best_model.predict(tta_img)\n",
    "print(tta_res.shape)\n",
    "print(np.sum(tta_res,axis=0))\n",
    "\n",
    "test_y = []\n",
    "for img_p in test_files:\n",
    "    tmp_x = get_tta_img(img_p)\n",
    "    tmp_y = best_model.predict(tmp_x)\n",
    "    tmp_y = np.sum(tmp_y,axis=0)\n",
    "    test_y.append(tmp_y)\n",
    "\n",
    "test_y = np.array(test_y)\n",
    "print(test_y[:5])\n",
    "\n",
    "y_res = np.argmax(test_y,axis=1)\n",
    "y_res = [list_classes[i] for i in y_res]\n",
    "df = pd.read_csv('../input/sample_submission.csv')\n",
    "f_name = [p.split('/')[-1] for p in test_files]\n",
    "df['fname'] = f_name\n",
    "df['camera'] = y_res\n",
    "print(df.head())\n",
    "df.to_csv('../results/resnet_manip_tta.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
